# -*- coding: utf-8 -*-
"""script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i2GnO7zNGJgAKcb1d_LM3sFXd7uYehaD

#### Specific Version's Packages Installation
"""

#!pip install -r requirements.txt

"""#### Import Packages for use"""

import os
import sys
from glob import glob
import copy
import warnings
import time

from pomegranate import *
import pickle
import optuna

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import pandas as pd
import seaborn as sns
import librosa
import math
from tqdm import tqdm
from mpl_toolkits.mplot3d import Axes3D

from customGNB import *

import sklearn
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.dummy import DummyClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, recall_score
from sklearn.metrics import f1_score
from sklearn.model_selection import cross_val_score
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

import torch
from torch.utils.data import DataLoader, Dataset
import torch.optim as optim
from torch import nn
from torch.nn.parameter import Parameter
from torch.autograd import Variable
from torch.nn import init

"""## Usefull functions"""

# function that reads wav files
def read_wav(f):
    wav, _ = librosa.core.load(f, sr=None)
    return wav

def dataparser(directory):

    # Parse relevant dataset info
    files = glob(os.path.join(directory, "*.wav"))
    fnames = np.array([f.split("/")[-1].split(".")[0] for f in files])

    map_digits = {'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9} # dictionary to many string to int

    digits = [] # list to append spoken digits
    speakers = [] # list to append speaker's id
    wavs = [] # list to append wavs files
    for f in fnames:
      digits.append(map_digits[''.join(i for i in f if not i.isdigit())]) # remove numeric digits
      speakers.append(int(''.join(c for c in f if c.isdigit()))) # keep only numeric digits
      wavs.append(read_wav('./digits/{}.wav'.format(f))) # read wav file

    _, Fs = librosa.core.load(files[0], sr=None) # find Fs from librosa

    # Print dataset info
    print("Total wavs: {}. Fs = {} Hz".format(len(wavs), Fs))
    return wavs, np.array(speakers), np.array(digits)   # 'wavs' is list, because of the difference of the signal duration(mfccs matrix is W x 13)
                                                        # 'speakers' and 'digits' are np.arrays

"""We call the dataparser and store our data"""

wavs, speakers, digits = dataparser('./digits')

"""## Section 0 - MFCC Extraction

We declare a funcion for feature extraction
"""

def extract_features(wavs, w_sec, s_sec, n_mfcc=13, Fs=16000,):
    window = int(w_sec * Fs) # window to be used
    step = int(s_sec * Fs)  # step to be used

    mfcc_deltas=[]  # list to append deltas
    mfcc_delta2s=[] # list to append delta-deltas

    # Extract MFCCs for all wavs
    mfccs = [
        librosa.feature.mfcc(
            y=wav, sr=Fs, win_length=window, hop_length=step, n_mfcc=n_mfcc
        ).T #This is a array W x 13
        for wav in tqdm(wavs, desc="Extracting features...")
    ]

    for mfcc in mfccs:
        mfcc_deltas.append(librosa.feature.delta(mfcc)) # get delta for each mfcc
        mfcc_delta2s.append(librosa.feature.delta(mfcc, order=2)) # get delta-deltas for each mfcc

    print("Feature extraction completed with {} mfccs per frame".format(n_mfcc))

    return mfccs, mfcc_deltas, mfcc_delta2s  #These are lists, because of the difference of the signal duration(mfccs matrix is W x 13)

# Parameters Initialization
w_sec = 0.025 #sec
s_sec = 0.01 #sec
n_mfcc = 13
Fs = 16000 #Hz
mfccs, mfcc_deltas, mfcc_delta2s = extract_features(wavs, w_sec, s_sec, n_mfcc = n_mfcc, Fs=Fs)

print(mfccs[0].shape)

"""## Section 1 - Features Histograms / Correlation Matices

a) MFCCs Histogramms
"""

def hist(mfccs, digit, subtitle):

    fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(20,20))
    for mfcc, ax, utter in list(zip(mfccs, axes.flatten(), np.arange(mfccs.shape[0]))):
        n, bins, patches = ax.hist(x=mfcc, bins='auto', color='#0504aa',alpha=0.5, rwidth=0.85)
        ax.grid(axis='y', alpha=0.75)
        ax.set_xlabel('mfc coefficient values')
        ax.set_ylabel(' frequency ')
        ax.set_title(subtitle + f'of Utterance {utter} of digit {digit}')

        maxval = n.max()
        # Set a clean upper y-axis limit.
        ax.set_ylim(ymax=np.ceil(maxval / 10) * 10 if maxval % 10 else maxval + 10)
    fig.tight_layout()
    plt.show()

print(type(mfccs[1][:2]))

n1 = 6
n2 = 9

# n1
indxs1 = np.where(digits==n1)[0]
mfccs_n1 = np.array([mfccs[indx][:2] for indx in indxs1]) #This is an array of arrays of (2 X 13)

# n2
indxs2 = np.where(digits==n2)[0]
mfccs_n2 = np.array([mfccs[indx][:2] for indx in indxs2]) #This is an array of arrays of (2 X 13)

print(mfccs_n2.shape)

hist(mfccs_n1[:,0], 'Six', 'First Coeff ')
print('\n')
hist(mfccs_n1[:,1], 'Six', 'Second Coeff ')
print('\n')
hist(mfccs_n2[:,0], 'Nine', 'First Coeff ')
print('\n')
hist(mfccs_n2[:,1], 'Nine', 'Second Coeff ')

"""b) MFSCs"""

def extract_MFSCs(wavs, w_sec, s_sec, n_mels=13, Fs=16000,):
    # Extract MFCCs for all wavs
    window = int(w_sec * Fs)
    step = int(s_sec * Fs)

    mfscs = [
      librosa.feature.melspectrogram(
            y=wav, sr=Fs, win_length=window, hop_length=step,
            n_mels=n_mels
        ).T #This is an array of arrays of (W X 13)
        for wav in tqdm(wavs, desc="Extracting MFSCs...")

    ]

    print("Feature extraction completed with {} mfscs per frame".format(n_mels))

    return mfscs  #This is a list, because of the difference of the signal duration(mfccs matrix is W x 13)

print(indxs1)

n1_subwavs = list([wavs[indx] for indx in indxs1][:2])
n2_subwavs = list([wavs[indx] for indx in indxs2][:2])
n_mels = 13 #== n_mels

print(len(n1_subwavs))
print(n1_subwavs[0].shape)

mfscs_n1 = extract_MFSCs(n1_subwavs, w_sec, s_sec, n_mels, Fs)  #This is a list, because of the difference of the signal duration(mfccs matrix is W x 13)
mfscs_n2 = extract_MFSCs(n2_subwavs, w_sec, s_sec, n_mels, Fs)

print(len(mfscs_n1))
print(mfscs_n1[0].shape)

"""### MFCCS Correlation Matrix

For N1 = 6
"""

# First Utterance
mfccs_n1 = [mfccs[indx] for indx in indxs1[:2]] #This is a list, because of the difference of the signal duration(mfccs matrix is W x 13)

n1_df_1 = pd.DataFrame(mfccs_n1[0])
plt.figure(figsize = (8, 8))
plt.title(f'Correlation Matrix for Digit {n1} and 1st Utterance')
sns.heatmap(n1_df_1.corr(), annot = True)
plt.show()
print('\n')

# Second Utterance
n1_df_2 = pd.DataFrame(mfccs_n1[1])
plt.figure(figsize = (8, 8))
plt.title(f'Correlation Matrix for Digit {n1} and 2nd Utterance')
ax = sns.heatmap(n1_df_2.corr(), annot = True)
plt.show()

"""For N2 = 9"""

# First Utterance
mfccs_n2 = [mfccs[indx] for indx in indxs2][:2] #This is a list, because of the difference of the signal duration(mfccs matrix is W x 13)

n2_df_1 = pd.DataFrame(mfccs_n2[0])
plt.figure(figsize = (8, 8))
plt.title(f'Correlation Matrix for Digit {n2} and 1st Utterance')
sns.heatmap(n2_df_1.corr(), annot = True)
plt.show()
print('\n')

# Second Utterance
n2_df_2 = pd.DataFrame(mfccs_n2[1])
plt.figure(figsize = (8, 8))
plt.title(f'Correlation Matrix for Digit {n2} and 2nd Utterance')
ax = sns.heatmap(n2_df_2.corr(), annot = True)
plt.show()

"""### MFSCS Correlation Matrix

For N1 = 6
"""

# First Utterance
n1_df_1 = pd.DataFrame(mfscs_n1[0])
plt.figure(figsize = (8, 8))
plt.title(f'Correlation Matrix for Digit {n1} and 1st Utterance')
sns.heatmap(n1_df_1.corr(), annot = True)
plt.show()
print('\n')

# Second Utterance
n1_df_2 = pd.DataFrame(mfscs_n1[1])
plt.figure(figsize = (8, 8))
plt.title(f'Correlation Matrix for Digit {n1} and 2nd Utterance')
ax = sns.heatmap(n1_df_2.corr(), annot = True)
plt.show()

"""For N2 = 9"""

# First Utterance
n2_df_1 = pd.DataFrame(mfscs_n2[0])
plt.figure(figsize = (8, 8))
plt.title(f'Correlation Matrix for Digit {n2} and 1st Utterance')
sns.heatmap(n2_df_1.corr(), annot = True)
plt.show()
print('\n')

# Second Utterance
n2_df_2 = pd.DataFrame(mfscs_n2[1])
plt.figure(figsize = (8, 8))
plt.title(f'Correlation Matrix for Digit {n2} and 2nd Utterance')
ax = sns.heatmap(n2_df_2.corr(), annot = True)
plt.show()

"""## Section 2 - Data Dimensionality Reduction"""

means = np.zeros((133, 39))
stds = np.zeros((133, 39))
for i in range (len(mfccs)):
  sample = np.concatenate((mfccs[i],mfcc_deltas[i],mfcc_delta2s[i]),axis=1) # (X,39), where X:unkonw
  mean = np.mean(sample,axis=0)
  std = np.std(sample,axis=0)
  means[i] = mean
  stds[i] = std


print(means.shape)#(133,39)
print(stds.shape) #(133,39)

markers = ['o', 's', 'v', '*', 'o', 's', 'v', '*','o']
# colors = cm.rainbow(np.linspace(0, 1, len(markers)))
colors = ['red', 'green', 'brown', 'blue', 'yellow', 'orange', 'pink','gray', 'cyan']

plt.figure(figsize=(12,8))

for digit, marker, color in zip(set(digits), markers, colors):

  indxs = np.where(digits==digit)[0]
  mean = np.array([means[indx] for indx in indxs])

  plt.scatter(mean[:, 0], mean[:, 1], c=color, marker=marker, label=digit)

plt.legend()
plt.title('Scatter plot of "mean" features')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.show()

markers = ['o', 's', 'v', '*', 'o', 's', 'v', '*','o']
# colors = cm.rainbow(np.linspace(0, 1, len(markers)))
colors = ['red', 'green', 'brown', 'blue', 'yellow', 'orange', 'pink','gray', 'cyan']

plt.figure(figsize=(12,8))

for digit, marker, color in zip(set(digits), markers, colors):

  indxs = np.where(digits==digit)[0]
  std = np.array([stds[indx] for indx in indxs])

  plt.scatter(std[:, 0], std[:, 1], c=color, marker=marker, label=digit)

plt.legend()
plt.title('Scatter plot of "std" features')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.show()

"""### 2D PCA"""

pca_mean = PCA(n_components=2)
pca_std = PCA(n_components=2)

_means = pca_mean.fit_transform(means)
_stds = pca_std.fit_transform(stds)

markers = ['o', 's', 'v', '*', 'o', 's', 'v', '*','o']
# colors = cm.rainbow(np.linspace(0, 1, len(markers)))
colors = ['red', 'green', 'brown', 'blue', 'yellow', 'orange', 'pink','gray', 'cyan']

plt.figure(figsize=(12,8))

for digit, marker, color in zip(set(digits), markers, colors):

  indxs = np.where(digits==digit)[0]
  _mean = np.array([_means[indx] for indx in indxs])

  plt.scatter(_mean[:, 0], _mean[:, 1], c=color, marker=marker, label=digit)

plt.legend()
plt.title('Scatter plot of "mean" features')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.show()

markers = ['o', 's', 'v', '*', 'o', 's', 'v', '*','o']
# colors = cm.rainbow(np.linspace(0, 1, len(markers)))
colors = ['red', 'green', 'brown', 'blue', 'yellow', 'orange', 'pink','gray', 'cyan']

plt.figure(figsize=(12,8))

for digit, marker, color in zip(set(digits), markers, colors):

  indxs = np.where(digits==digit)[0]
  _std = np.array([_stds[indx] for indx in indxs])

  plt.scatter(_std[:, 0], _std[:, 1], c=color, marker=marker, label=digit)

plt.legend()
plt.title('Scatter plot of "std" features')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.show()

print(pca_mean.explained_variance_ratio_)
print(pca_std.explained_variance_ratio_)

"""### 3D PCA"""

pca_mean = PCA(n_components=3)
pca_std = PCA(n_components=3)

_means3 = pca_mean.fit_transform(means)
_stds3 = pca_std.fit_transform(stds)

fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(projection='3d')

markers = ['o', 's', 'v', '*', 'o', 's', 'v', '*','o']
# colors = cm.rainbow(np.linspace(0, 1, len(markers)))
colors = ['red', 'green', 'brown', 'blue', 'yellow', 'orange', 'pink','gray', 'cyan']


for digit, marker, color in zip(set(digits), markers, colors):

  indxs = np.where(digits==digit)[0]
  _mean3 = np.array([_means3[indx] for indx in indxs])

  ax.scatter(_mean3[:, 0], _mean3[:, 1], _mean3[:, 2], c=color, marker=marker, label=digit)

plt.legend(loc='best')
ax.set_xlabel('Dimension 1')
ax.set_ylabel('Dimension 2')
ax.set_zlabel('Dimension 3')
ax.set_title('Mean Value in 3 Dimensions')
plt.show()

fig = plt.figure(figsize=(12,8))
ax = fig.add_subplot(projection='3d')

markers = ['o', 's', 'v', '*', 'o', 's', 'v', '*','o']
# colors = cm.rainbow(np.linspace(0, 1, len(markers)))
colors = ['red', 'green', 'brown', 'blue', 'yellow', 'orange', 'pink','gray', 'cyan']


for digit, marker, color in zip(set(digits), markers, colors):

  indxs = np.where(digits==digit)[0]
  _std3 = np.array([_stds3[indx] for indx in indxs])

  ax.scatter(_std3[:, 0], _std3[:, 1], _std3[:, 2], c=color, marker=marker, label=digit)

plt.legend(loc='best')
ax.set_xlabel('Dimension 1')
ax.set_ylabel('Dimension 2')
ax.set_zlabel('Dimension 3')
ax.set_title('Std Value in 3 Dimensions')
plt.show()

print(pca_mean.explained_variance_ratio_)
print(pca_std.explained_variance_ratio_)

"""## Section 3 - Baselines for Digit Classification"""

def evaluate_clasifier(clf, X_train, y_train, X_test, y_test, folds=10):
    clf_clone = sklearn.base.clone(clf)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)  # calculate accuracy
    f1 = f1_score(y_test, y_pred, average='macro')  # calculate f1-score
    cross_val_acc = cross_val_score(clf_clone, X_train, y_train, cv=KFold(n_splits=folds), scoring="accuracy")  # calculate k-fold-cross-validation accuracy score
    cross_val_f1 = cross_val_score(clf_clone, X_train, y_train, cv=KFold(n_splits=folds), scoring="f1_macro")  # calculate k-fold-cross-validation f1 score
    return acc, f1, cross_val_acc, cross_val_f1


def Classifiers_evaluation(X_train, y_train, X_test, y_test, clfs=[CustomNBClassifier(), GaussianNB(), DummyClassifier(), KNeighborsClassifier(n_neighbors=5), LogisticRegression()]):
  warnings.filterwarnings('ignore') # ignore warning messages
  scores_default = {}
  for clf in clfs:
      s1, s2, s3, s4 = evaluate_clasifier(clf, X_train, y_train, X_test, y_test, folds=10)
      scores_default[str(clf).split('(')[0]] = [s1, s2, np.mean(s3), np.std(s3), np.mean(s4), np.std(s4)]
  return scores_default


def display_results(X_train, y_train, X_test, y_test, clfs=[CustomNBClassifier(), GaussianNB(), DummyClassifier(), KNeighborsClassifier(n_neighbors=5), LogisticRegression()]):
  scores_default = Classifiers_evaluation(X_train, y_train, X_test, y_test, clfs)

  columns_labels = ['Classifier', 'Accuracy', 'F1-Score', '10-Fold Cross-Validation-Accuracy-Score', '10-Fold Cross-Validation-F1-Score']

  clfs_names = [str(clfs[0]).split('(')[0], str(clfs[1]).split('(')[0], str(clfs[2]).split('(')[0], str(clfs[3]).split('(')[0], str(clfs[4]).split('(')[0]]

  acc = [scores_default[clfs_names[0]][0], scores_default[clfs_names[1]][0], scores_default[clfs_names[2]][0], scores_default[clfs_names[3]][0], scores_default[clfs_names[4]][0]]

  f1 = [scores_default[clfs_names[0]][1], scores_default[clfs_names[1]][1], scores_default[clfs_names[2]][1], scores_default[clfs_names[3]][1], scores_default[clfs_names[4]][1]]

  cross_val_acc_means = [scores_default[clfs_names[0]][2], scores_default[clfs_names[1]][2], scores_default[clfs_names[2]][2], scores_default[clfs_names[3]][2], scores_default[clfs_names[4]][2]]
  cross_val_acc_stds = [scores_default[clfs_names[0]][3], scores_default[clfs_names[1]][3], scores_default[clfs_names[2]][3], scores_default[clfs_names[3]][3], scores_default[clfs_names[4]][3]]

  cross_val_f1_means = [scores_default[clfs_names[0]][4], scores_default[clfs_names[1]][4], scores_default[clfs_names[2]][4], scores_default[clfs_names[3]][4], scores_default[clfs_names[4]][4]]
  cross_val_f1_stds = [scores_default[clfs_names[0]][5], scores_default[clfs_names[1]][5], scores_default[clfs_names[2]][5], scores_default[clfs_names[3]][5], scores_default[clfs_names[4]][5]]
  cross_val_acc = []
  cross_val_f1 = []
  for i in range(5):
      cross_val_acc.append(str(str(cross_val_acc_means[i]) + ' +- ' + str(cross_val_acc_stds[i])))
      cross_val_f1.append(str(str(cross_val_f1_means[i]) + ' +- ' + str(cross_val_f1_stds[i])))

  values = zip(clfs_names, acc, f1, cross_val_acc, cross_val_f1)

  cm = sns.light_palette("pink", as_cmap=True)

  df1 = pd.DataFrame(values, columns=columns_labels)
  pd.options.display.float_format = "{:.3f}".format

  df1.style.set_caption("Scores of Classifiers")\
      .background_gradient(cmap=cm)

  display(df1)

"""*   1st Implementation [Vertical Stacking] -- X_data : (266x39)"""

# create dataset
X_data = StandardScaler().fit_transform(np.concatenate((means,stds)))
y_data = np.concatenate((digits,digits))
print('Initial-dataset shape: {}'.format(X_data.shape))

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3) # slice dataset using function from scikit-learn
print('Train-set shape: {}, {}'.format(X_train.shape, y_train.shape)) # print the shapes for testing
print('Test-set shapes: {}, {}'.format(X_test.shape, y_test.shape)) # print the shapes for testing

display_results(X_train, y_train, X_test, y_test, clfs=[CustomNBClassifier(), GaussianNB(), DummyClassifier(), KNeighborsClassifier(n_neighbors=5), LogisticRegression()])

"""*   2nd Implementation [Vertical Stacking with PCA] -- X_data : (266x5)"""

X_data = PCA(n_components=5).fit_transform(X_data)
print('Initial-dataset shape: {}'.format(X_data.shape))

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3) # slice dataset using function from scikit-learn
print('Train-set shape: {}, {}'.format(X_train.shape, y_train.shape)) # print the shapes for testing
print('Test-set shapes: {}, {}'.format(X_test.shape, y_test.shape)) # print the shapes for testing

display_results(X_train, y_train, X_test, y_test, clfs=[CustomNBClassifier(), GaussianNB(), DummyClassifier(), KNeighborsClassifier(n_neighbors=5), LogisticRegression()])

"""*   3rd Implementation [Horizontal Stacking] -- X_data : (133x78)"""

# create dataset
X_data = StandardScaler().fit_transform(np.concatenate((means,stds),axis=-1))
y_data = digits
print('Initial-dataset shape: {}'.format(X_data.shape))

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3) # slice dataset using function from scikit-learn
print('Train-set shape: {}, {}'.format(X_train.shape, y_train.shape)) # print the shapes for testing
print('Test-set shapes: {}, {}'.format(X_test.shape, y_test.shape)) # print the shapes for testing

display_results(X_train, y_train, X_test, y_test, clfs=[CustomNBClassifier(), GaussianNB(), DummyClassifier(), KNeighborsClassifier(n_neighbors=5), LogisticRegression()])

"""*   4th Implementation [Horizontal Stacking with PCA] -- X_data : (133x5)"""

X_data = PCA(n_components=5).fit_transform(X_data)
print('Initial-dataset shape: {}'.format(X_data.shape))

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3) # slice dataset using function from scikit-learn
print('Train-set shape: {}, {}'.format(X_train.shape, y_train.shape)) # print the shapes for testing
print('Test-set shapes: {}, {}'.format(X_test.shape, y_test.shape)) # print the shapes for testing

display_results(X_train, y_train, X_test, y_test, clfs=[CustomNBClassifier(), GaussianNB(), DummyClassifier(), KNeighborsClassifier(n_neighbors=5), LogisticRegression()])

"""*   5th Implementation [Extra Feautures / Zero Crossing Rate] -- X_data : (266x40)"""

# Add a new feature to data

means_new = np.zeros((133, 40))
stds_new = np.zeros((133, 40))

for i in range (len(mfccs)):
  zcrs = librosa.feature.zero_crossing_rate(wavs[i],frame_length=400, hop_length=160)
  sample = np.concatenate((mfccs[i],mfcc_deltas[i],mfcc_delta2s[i],zcrs.T),axis=1) # (X,40), where X:unkonw
  mean = np.mean(sample,axis=0)
  std = np.std(sample,axis=0)
  means_new[i] = mean
  stds_new[i] = std

print(means_new.shape) #(133,40)
print(stds_new.shape) #(133,40)

# create dataset
X_data = StandardScaler().fit_transform(np.concatenate((means_new,stds_new)))
y_data = np.concatenate((digits,digits))
print('Initial-dataset shape: {}'.format(X_data.shape))

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.3) # slice dataset using function from scikit-learn
print('Train-set shape: {}, {}'.format(X_train.shape, y_train.shape)) # print the shapes for testing
print('Test-set shapes: {}, {}'.format(X_test.shape, y_test.shape)) # print the shapes for testing

display_results(X_train, y_train, X_test, y_test, clfs=[CustomNBClassifier(), GaussianNB(), DummyClassifier(), KNeighborsClassifier(n_neighbors=5), LogisticRegression()])