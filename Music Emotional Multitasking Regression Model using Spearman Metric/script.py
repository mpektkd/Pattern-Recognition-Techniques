# -*- coding: utf-8 -*-
"""script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11GVvyPlPk7DBF3wCOldcsbToKzoUtZTk

## Section 0 - Download / Reduce Dataset
"""

from google.colab import drive
drive.mount('/content/drive')

! pip install kaggle
! mkdir ~/.kaggle

!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json

! kaggle datasets download -d geoparslp/patreco3-multitask-affective-music

! unzip /content/patreco3-multitask-affective-music.zip

from lib import *

"""### Parameters Initialization"""

# Combine similar classes and remove underrepresented classes
class_mapping = {
    'Rock': 'Rock',
    'Psych-Rock': 'Rock',
    'Indie-Rock': None,
    'Post-Rock': 'Rock',
    'Psych-Folk': 'Folk',
    'Folk': 'Folk',
    'Metal': 'Metal',
    'Punk': 'Metal',
    'Post-Punk': None,
    'Trip-Hop': 'Trip-Hop',
    'Pop': 'Pop',
    'Electronic': 'Electronic',
    'Hip-Hop': 'Hip-Hop',
    'Classical': 'Classical',
    'Blues': 'Blues',
    'Chiptune': 'Electronic',
    'Jazz': 'Jazz',
    'Soundtrack': None,
    'International': None,
    'Old-Time': None
}

"""### Plot Histograms"""

# function for histogram plotting
def create_histogram(file_path, graph_name, mapping=False):
    with open (file_path, 'r') as f:
        lines = f.readlines() # read file
    lines.pop(0) # remove head line
    if (mapping==True):
        categories = [class_mapping[l.split('\t')[-1].strip()] for l in lines] # keep only categories after mapping
    else:
        categories = [l.split('\t')[-1].strip() for l in lines] # keep only categories
    cat_dict = {i:categories.count(i) for i in categories} # count the instances
    if (mapping==True):
        del cat_dict[None] # delete None if mapping was applied

    # plot the histogram
    fig = plt.figure(figsize=(20,5))
    plt.bar(list(cat_dict.keys()), list(cat_dict.values()), color ='skyblue',width = 0.2) # creating the bar plot
    plt.xlabel("Categories")
    plt.ylabel("Quantity")
    plt.title(graph_name)
    plt.show()

"""#### Training Set"""

file_path = 'data/fma_genre_spectrograms/train_labels.txt'
create_histogram(file_path, graph_name="Histogram before Mapping (train-set)", mapping=False)
create_histogram(file_path, graph_name="Histogram after Mapping (train-set)", mapping=True)

"""#### Test Set"""

file_path = 'data/fma_genre_spectrograms/test_labels.txt'
create_histogram(file_path, graph_name="Histogram before Mapping (test-set)", mapping=False)
create_histogram(file_path, graph_name="Histogram after Mapping (test-set)", mapping=True)

"""## Section 1 - LSTM Classification

### Define General Training Parameters
"""

# Define training parameters
val_size = 0.2
batch_size = 32
BATCH_SIZE = batch_size
EPOCHS = 100
ETA = 1e-4
OUTPUT_SIZE = 10
HIDDEN_LAYER = 50
NUM_LAYERS = 1

# Define model hyperparameters
layer_channels = [(1, 4), (4, 16), (16, 64), (64, 256)]
hidden_features = 50
out_features = 10
kernels = [5, 4, 3, 3]

# Define the additional training parameter for bidirectional LSTM
bidirectional = True

# Define the additional training parameter for pack_padded_sequence implementation
pack_padded_sequence = False

# Define the additional training parameters
weight_decay = 1e-6
dropout = .2

# Define the additional training parameter for early tolerance
PATIENCE = 10

#Define out LossFunction
criterion = nn.CrossEntropyLoss()

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Feats Initialization
mels_feats = 128
chroma_feats = 12
fused_feats = mels_feats + chroma_feats

"""### Training and Evalution using 'beat-synced datasets'

####  Mel Spectrograms Dataset
"""

data = SpectrogramDataset("data/fma_genre_spectrograms_beat",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, max_length=data.max_length)

"""##### Overfit the model

"""

EarlyLSTMNet = LSTM(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

clf_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, train_loader, EPOCHS, PATIENCE=PATIENCE, overfit_batch=True)

"""##### Normal Training"""

EarlyLSTMNet = LSTM(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyLSTM_beatMEL', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_beatMEL.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, mels_feats, E_model, criterion)

print(classification_report(y_test_gold, y_test_pred))

"""#### Chromagrams Dataset"""

data = SpectrogramDataset("data/fma_genre_spectrograms_beat",
                                  class_mapping=class_mapping, train=True, read_spec_fn=read_chromagram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms_beat/',
                                              class_mapping=class_mapping, train=False, read_spec_fn=read_chromagram, max_length=data.max_length)

"""##### Normal Training"""

EarlyLSTMNet = LSTM(input_dim=chroma_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyLSTMNet, chroma_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyLSTM_beatCHROMA', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_beatCHROMA.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, chroma_feats, E_model, criterion)

print(classification_report(y_test_gold, y_test_pred))

"""### Mel Spectrograms + Chromagrams Dataset"""

data = SpectrogramDataset("data/fma_genre_spectrograms_beat",
                                class_mapping=class_mapping, train=True, read_spec_fn=read_fused_spectrogram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms_beat/',
                                            class_mapping=class_mapping, train=False, read_spec_fn=read_fused_spectrogram, max_length=data.max_length)

"""#### Normal Training"""

EarlyLSTMNet = LSTM(input_dim=fused_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyLSTMNet, fused_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyLSTM_beatFUSED', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_beatFUSED.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, fused_feats, E_model, criterion)

print(classification_report(y_test_gold, y_test_pred))

"""### Training and Evalution using 'non-beat-synced datasets'

####  Mel Spectrograms Dataset
"""

data = SpectrogramDataset("data/fma_genre_spectrograms",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, max_length=data.max_length)

"""##### Normal Training"""

EarlyLSTMNet = LSTM(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyLSTM_nonbeatMEL', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_nonbeatMEL.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, mels_feats, E_model, criterion)

print(classification_report(y_test_gold, y_test_pred))

"""#### Chromagrams Dataset"""

data = SpectrogramDataset("data/fma_genre_spectrograms",
                                  class_mapping=class_mapping, train=True, read_spec_fn=read_chromagram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms/',
                                              class_mapping=class_mapping, train=False, read_spec_fn=read_chromagram, max_length=data.max_length)

"""##### Normal Training"""

EarlyLSTMNet = LSTM(input_dim=chroma_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyLSTMNet, chroma_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyLSTM_nonbeatCHROMA', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_nonbeatCHROMA.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, chroma_feats, E_model, criterion)

print(classification_report(y_test_gold, y_test_pred))

"""#### Mel Spectrograms + Chromagrams Dataset"""

data = SpectrogramDataset("data/fma_genre_spectrograms",
                                class_mapping=class_mapping, train=True, read_spec_fn=read_fused_spectrogram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms/',
                                            class_mapping=class_mapping, train=False, read_spec_fn=read_fused_spectrogram, max_length=data.max_length)

"""##### Normal Training"""

EarlyLSTMNet = LSTM(input_dim=fused_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyLSTMNet, fused_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyLSTM_nonbeatFUSED', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_nonbeatFUSED.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, fused_feats, E_model, criterion)

print(classification_report(y_test_gold, y_test_pred))

"""## Section 2 - 2D CNN Classification

### Training and Evalution using 'beat-synced datasets'

####  Mel Spectrograms Dataset
"""

data = SpectrogramDataset("data/fma_genre_spectrograms_beat",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, max_length=data.max_length)

"""##### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels, dropout=dropout).to(device)
print(EarlyCNNNet)
# Make instance of our optimizer, with L2-Reguralization
# optimizer = optim.SGD(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyCNNNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyCNN_beatMEL', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_beatMEL.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, mels_feats, E_model, criterion, cnn=True)

print(classification_report(y_test_gold, y_test_pred))

"""#### Chromagrams Dataset"""

data = SpectrogramDataset("data/fma_genre_spectrograms_beat",
                                  class_mapping=class_mapping, train=True, read_spec_fn=read_chromagram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms_beat/',
                                              class_mapping=class_mapping, train=False, read_spec_fn=read_chromagram, max_length=data.max_length)

"""##### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels, dropout=dropout, chroma=True).to(device)
print(EarlyCNNNet)
# Make instance of our optimizer, with L2-Reguralization
# optimizer = optim.SGD(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyCNNNet, chroma_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyCNN_beatCHROMA', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_beatCHROMA.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, chroma_feats, E_model, criterion, cnn=True)

print(classification_report(y_test_gold, y_test_pred))

"""#### Mel Spectrograms + Chromagrams Dataset"""

data = SpectrogramDataset("data/fma_genre_spectrograms_beat",
                                class_mapping=class_mapping, train=True, read_spec_fn=read_fused_spectrogram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms_beat/',
                                            class_mapping=class_mapping, train=False, read_spec_fn=read_fused_spectrogram, max_length=data.max_length)

"""##### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels, dropout=dropout).to(device)
print(EarlyCNNNet)
# Make instance of our optimizer, with L2-Reguralization
# optimizer = optim.SGD(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyCNNNet, fused_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyCNN_beatFUSED', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_beatFUSED.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, fused_feats, E_model, criterion, cnn=True)

print(classification_report(y_test_gold, y_test_pred))

"""### Training and Evalution using 'non-beat-synced datasets'

####  Mel Spectrograms Dataset
"""

data = SpectrogramDataset("data/fma_genre_spectrograms",
                                  class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms/',
                                              class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, max_length=data.max_length)

"""##### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels, dropout=dropout).to(device)
print(EarlyCNNNet)
# Make instance of our optimizer, with L2-Reguralization
# optimizer = optim.SGD(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyCNNNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyCNN_nonbeatMEL', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_nonbeatMEL.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, mels_feats, E_model, criterion, cnn=True)

print(classification_report(y_test_gold, y_test_pred))

"""#### Chromagrams Dataset"""

data = SpectrogramDataset("data/fma_genre_spectrograms",
                                    class_mapping=class_mapping, train=True, read_spec_fn=read_chromagram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms/',
                                                class_mapping=class_mapping, train=False, read_spec_fn=read_chromagram, max_length=data.max_length)

"""##### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels, dropout=dropout, chroma=True).to(device)
print(EarlyCNNNet)
# Make instance of our optimizer, with L2-Reguralization
# optimizer = optim.SGD(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyCNNNet, chroma_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyCNN_nonbeatCHROMA', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_nonbeatCHROMA.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, chroma_feats, E_model, criterion, cnn=True)

print(classification_report(y_test_gold, y_test_pred))

"""#### Mel Spectrograms + Chromagrams Dataset"""

data = SpectrogramDataset("data/fma_genre_spectrograms_beat",
                                    class_mapping=class_mapping, train=True, read_spec_fn=read_fused_spectrogram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms/',
                                                class_mapping=class_mapping, train=False, read_spec_fn=read_fused_spectrogram, max_length=data.max_length)

"""##### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels, dropout=dropout).to(device)
print(EarlyCNNNet)
# Make instance of our optimizer, with L2-Reguralization
# optimizer = optim.SGD(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyCNNNet, fused_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyCNN_nonbeatFUSED', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_beatFUSED.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, fused_feats, E_model, criterion, cnn=True)

print(classification_report(y_test_gold, y_test_pred))

"""## Section 3 - Regression Model (Experiment with Spearman Loss)

### Spearman Correlation
"""

from fast_soft_sort.pytorch_ops import soft_rank

class SpearmanLoss(nn.Module):

    def __init__(self, regularization="l2", regularization_strength=1.0):
        super(SpearmanLoss, self).__init__()
        self.regularization = regularization
        self.regularization_strength = regularization_strength
        self.criterion = nn.MSELoss()

    def forward(self, pred, target):
      # fast_soft_sort uses 1-based indexing, divide by len to compute percentage of rank
        # print(pred.cpu().reshape(1, -1).shape)
        self.mse = self.criterion(pred, target)
        pred = soft_rank(
            pred.cpu().reshape(1, -1),
            regularization = self.regularization,
            regularization_strength = self.regularization_strength,
        )
        # print(pred.shape)
        pred = pred.cuda()
        return self.corrcoef(pred / pred.shape[-1], target)

    def corrcoef(self, pred, target):
        # np.corrcoef in torch from @mdo
        # https://forum.numer.ai/t/custom-loss-functions-for-xgboost-using-pytorch/960
        pred_n = pred - pred.mean()
        target_n = target - target.mean()
        pred_n = pred_n / pred_n.norm()
        target_n = target_n / target_n.norm()

        return -0.1*(pred_n * target_n).sum() + self.mse

"""### Regression Training

#### Multi Parameters
"""

# Create map for sentiments
mapping = {
    'valence' : 1,
    'energy' : 2,
    'danceability' : 3,
}

"""### LSTM

#### Redefine LSTM Regression Parameters
"""

# Define training parameters
batch_size = 32
BATCH_SIZE = batch_size
OUTPUT_SIZE = 1
EPOCHS = 100
HIDDEN_LAYER = 50
regularization_strength = 1e-2
criterion = SpearmanLoss(regularization_strength=regularization_strength)
weight_decay = 1e-7
NUM_LAYERS = 2
PATIENCE = 10
dropout = 0.2

"""####  Emotion "Valence"
"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'], max_length = data.max_length)

"""##### Overfit the model"""

EarlyLSTMNet = LSTM2(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.SGD(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

reg_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, train_loader, 30, Descrs = ['Valence'], PATIENCE=PATIENCE, overfit_batch=True)

"""##### Normal Training"""

EarlyLSTMNet = LSTM2(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# criterion = SpearmanLoss(EarlyLSTMNet, 0.3)
# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs = ['Valence'], net_name='EarlyLSTM_multibeatVal', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_multibeatVal.pickle', 'rb') as handle:
    E_model = pickle.load(handle)
print(E_model)
# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, titles=['Valence'])

"""####  Emotion "Energy"
"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['energy'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['energy'])

"""##### Normal Training"""

EarlyLSTMNet = LSTM2(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs = ['Energy'], net_name='EarlyLSTM_multibeatEn', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_multibeatEn.pickle', 'rb') as handle:
    E_model = pickle.load(handle)
print(E_model)
# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, titles=['Energy'])

"""####  Emotion "Danceability"
"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['danceability'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['danceability'])

"""##### Normal Training"""

EarlyLSTMNet = LSTM2(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs=['Danceability'], net_name='EarlyLSTM_multibeatDanc', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_multibeatDanc.pickle', 'rb') as handle:
    E_model = pickle.load(handle)
print(E_model)
# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, titles=['Danceability'])

"""### LSTM - Attention

#### Redefine LSTM Regression Parameters
"""

# Define training parameters
batch_size = 32
BATCH_SIZE = batch_size
OUTPUT_SIZE = 1
EPOCHS = 100
HIDDEN_LAYER = 50
regularization_strength = 1e-2
criterion = SpearmanLoss(regularization_strength=regularization_strength)
weight_decay = 1e-7
NUM_LAYERS = 2
PATIENCE = 10
dropout = 0.2

"""####  Emotion "Valence"
"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'], max_length = data.max_length)

"""##### Normal Training"""

EarlyLSTMNet = LSTM2(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence,
                     att=True).to(device)

# criterion = SpearmanLoss(EarlyLSTMNet, 0.3)
# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs = ['Valence'], net_name='EarlyLSTM_multibeatVal', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_multibeatVal.pickle', 'rb') as handle:
    E_model = pickle.load(handle)
print(E_model)
# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, titles=['Valence'])

"""####  Emotion "Energy"
"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['energy'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['energy'])

"""##### Normal Training"""

EarlyLSTMNet = LSTM2(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence,
                     att=True).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs = ['Energy'], net_name='EarlyLSTM_multibeatEn', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_multibeatEn.pickle', 'rb') as handle:
    E_model = pickle.load(handle)
print(E_model)
# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, titles=['Energy'])

"""####  Emotion "Danceability"
"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['danceability'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['danceability'])

"""##### Normal Training"""

EarlyLSTMNet = LSTM2(input_dim=mels_feats,
                    rnn_size=HIDDEN_LAYER,
                    output_dim=OUTPUT_SIZE,
                    num_layers=NUM_LAYERS,
                    dropout=dropout,
                    pack_padded_sequence=pack_padded_sequence,
                     att=True).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyLSTMNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyLSTMNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs=['Danceability'], net_name='EarlyLSTM_multibeatDanc', PATIENCE=PATIENCE)

with open('./best_EarlyLSTM_multibeatDanc.pickle', 'rb') as handle:
    E_model = pickle.load(handle)
print(E_model)
# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, titles=['Danceability'])

"""### CNN

#### Redefine CNN Regression Parameters
"""

# Define training parameters
batch_size = 32
BATCH_SIZE = batch_size
out_features = 1
EPOCHS = 100
regularization_strength = 1e-2
criterion = SpearmanLoss(regularization_strength=regularization_strength)
weight_decay = 1e-5
PATIENCE = 10
dropout = 0.2

# Define model hyperparameters
layer_channels = [(1, 4), (4, 8), (8, 16), (16, 32)]
hidden_features = 50
kernels = [5, 5, 5, 5]

"""####  Emotion "Valence"
"""

data = SpectrogramDataset("data/multitask_dataset/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'])

"""##### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels, dropout=dropout).to(device)
print(EarlyCNNNet)

#Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

#Start the training process
reg_main(device, EarlyCNNNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs = ['Valence'], net_name='EarlyCNN_multibeatVal', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_multibeatVal.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, cnn=True, titles=['Valence'])

"""####  Emotion "Energy"
"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['energy'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['energy'])

"""##### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyCNNNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs = ['Energy'], net_name='EarlyCNN_multibeatEn', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_multibeatEn.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, cnn=True, titles=['Energy'])

"""####  Emotion "Danceability"
"""

data = SpectrogramDataset("data/multitask_dataset/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['danceability'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['danceability'])

"""##### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyCNNNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs=['Danceability'], net_name='EarlyCNN_multibeatDanc', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_multibeatDanc.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, cnn=True, titles=['Danceability'])

"""## Section 4 - Transfer Learning

### Multi Parameters
"""

# Create map for sentiments
mapping = {
    'valence' : 1,
    'energy' : 2,
    'danceability' : 3,
}

"""### Redefine Transfer Classification Parameters"""

out_features = 10
criterion = nn.CrossEntropyLoss()

"""### Spearman Correlation"""

from fast_soft_sort.pytorch_ops import soft_rank

class SpearmanLoss(nn.Module):

    def __init__(self, regularization="l2", regularization_strength=1.0):
        super(SpearmanLoss, self).__init__()
        self.regularization = regularization
        self.regularization_strength = regularization_strength
        self.criterion = nn.MSELoss()

    def forward(self, pred, target):
      # fast_soft_sort uses 1-based indexing, divide by len to compute percentage of rank
        # print(pred.cpu().reshape(1, -1).shape)
        self.mse = self.criterion(pred, target)
        pred = soft_rank(
            pred.cpu().reshape(1, -1),
            regularization = self.regularization,
            regularization_strength = self.regularization_strength,
        )
        # print(pred.shape)
        pred = pred.cuda()
        return self.corrcoef(pred / pred.shape[-1], target)

    def corrcoef(self, pred, target):
        # np.corrcoef in torch from @mdo
        # https://forum.numer.ai/t/custom-loss-functions-for-xgboost-using-pytorch/960
        pred_n = pred - pred.mean()
        target_n = target - target.mean()
        pred_n = pred_n / pred_n.norm()
        target_n = target_n / target_n.norm()

        return -0.1*(pred_n * target_n).sum() + self.mse

"""### CNN Classification for Transfer Learning"""

data = SpectrogramDataset("data/fma_genre_spectrograms_beat",
                                class_mapping=class_mapping, train=True, read_spec_fn=read_fused_spectrogram)
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/fma_genre_spectrograms_beat/',
                                            class_mapping=class_mapping, train=False, read_spec_fn=read_fused_spectrogram, max_length=data.max_length)

"""#### Overfit the model"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels, dropout=dropout).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

clf_main(device, EarlyCNNNet, fused_feats, optimizer, criterion, train_loader, train_loader, 30, PATIENCE=PATIENCE, overfit_batch=True, cnn=True)

"""#### Normal Training"""

EarlyCNNNet = CNN(layer_channels, hidden_features, out_features, kernels, dropout=dropout).to(device)
print(EarlyCNNNet)
# Make instance of our optimizer, with L2-Reguralization
# optimizer = optim.SGD(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

# Start the training process
clf_main(device, EarlyCNNNet, fused_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, net_name='EarlyCNN_beatFUSED', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_beatFUSED.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
test_loss, y_test_gold, y_test_pred = clf_eval(device, test_loader, fused_feats, E_model, criterion, cnn=True)

print(classification_report(y_test_gold, y_test_pred))

"""### Redefine Transfer Regression Parameters"""

out_features = 1
regularization_strength = 1e-2
criterion = SpearmanLoss(regularization_strength=regularization_strength)

"""### Transfer Learning for Regression

#### All net parameters trainable
"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'])

"""##### Overfit the model"""

with open('./best_EarlyCNN_beatFUSED.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# Change the last Layer
num_feats = E_model.fc.fc2.in_features
# E_model.fc.fc2 = nn.Linear(num_feats, out_features)
E_model.fc = FullyConnectedLayer(hidden_features, hidden_features, out_features)
E_model = E_model.to(device)
print(E_model)
#Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(E_model.parameters(), lr=ETA, weight_decay=weight_decay)

# training
reg_main(device, E_model, mels_feats, optimizer, criterion, train_loader, train_loader, 30, Descrs = ['Transfer Learning Valence'], PATIENCE=PATIENCE, overfit_batch=True, cnn=True)

"""##### Normal Training"""

with open('./best_EarlyCNN_beatFUSED.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# Change the last Layer
E_model.fc = FullyConnectedLayer(hidden_features, hidden_features, out_features)
E_model = E_model.to(device)
print(E_model)
#Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(E_model.parameters(), lr=ETA, weight_decay=weight_decay)

# training
reg_main(device, E_model, mels_feats, optimizer, criterion, train_loader, dev_loader, 30, Descrs = ['Transfer Learning Valence'], net_name='EarlyCNN_transfbeatFUSEDVal', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_transfbeatFUSEDVal.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, cnn=True, titles=['Transfer Learning Valence'])

"""#### Only fullyconnected parameters trainable"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'])
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=mapping['valence'])

"""##### Overfit the model"""

with open('./best_EarlyCNN_beatFUSED.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# Change the last Layer
E_model.fc = FullyConnectedLayer(hidden_features, hidden_features, out_features)
E_model = E_model.to(device)
print(E_model)
#Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(E_model.parameters(), lr=ETA, weight_decay=weight_decay)

# training
reg_main(device, E_model, mels_feats, optimizer, criterion, train_loader, train_loader, 30, Descrs = ['Transfer Learning Valence'], PATIENCE=PATIENCE, overfit_batch=True, cnn=True)

"""##### Normal Training"""

with open('./best_EarlyCNN_beatFUSED.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# Change the last Layer
num_feats = E_model.fc.fc2.in_features
# E_model.fc.fc2 = nn.Linear(num_feats, out_features)
E_model.fc = FullyConnectedLayer(hidden_features, hidden_features, out_features)
E_model = E_model.to(device)
# print(E_model)

# Turn-off gradient for cnn layer
for param in list(E_model.conv_layer.parameters()):
    param.requires_grad = False
parameters = [parameter for parameter in list(E_model.parameters()) if parameter.requires_grad==True]

# #Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(parameters, lr=ETA, weight_decay=weight_decay)

# # training
reg_main(device, E_model, mels_feats, optimizer, criterion, train_loader, dev_loader, 30, Descrs = ['Transfer Learning Valence'], net_name='EarlyCNN_transfbeatFUSEDValfreeze', PATIENCE=PATIENCE, cnn=True)

with open('./best_EarlyCNN_transfbeatFUSEDValfreeze.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, cnn=True, titles=['Transfer Learning Valence'])

"""## Multi Tasking

### Multi Parameters
"""

# Create map for sentiments
mapping = {
    'valence' : 1,
    'energy' : 2,
    'danceability' : 3,
}

"""### Multi CNN Training with trainable weights

The first experiment consists of a weighted multiloss of 3 SpearmanLoss. Each SpearmanLoss is a linear combination of -spearcorr, mse as it is shown in lib.py. In the present case, we train the weights of the 3 losses.

#### Redefine MultiTask Regresion Parameters
"""

out_features = 1
task_num = 3
regularization_strength = 1e-2
loss = SpearmanLoss(regularization_strength=regularization_strength)
criterion = MultiTaskLossTrain(task_num, loss)

"""#### Modeling Pipeline"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=np.array(list(mapping.values())))
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=np.array(list(mapping.values())))

"""##### Overfit the model

"""

EarlyCNNNet = MultiCNN(layer_channels, hidden_features, out_features, kernels).to(device)

# Make instance of our optimizer, with L2-Reguralization
criterion = MultiTaskLossTrain(task_num, loss)
parameters = list(EarlyCNNNet.parameters()) + list(criterion.parameters())
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

reg_main(device, EarlyCNNNet, mels_feats, optimizer, criterion, train_loader, train_loader, 30, Descrs = ['Valence', 'Energy', 'Danceability'], PATIENCE=PATIENCE, overfit_batch=True, cnn=True, multi=True)

"""##### Normal Training"""

EarlyCNNNet = MultiCNN(layer_channels, hidden_features, out_features, kernels).to(device)

# Make instance of our optimizer, with L2-Reguralization
parameters = list(EarlyCNNNet.parameters()) + list(criterion.parameters())
optimizer = optim.Adam(parameters, lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyCNNNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs = ['Valence', 'Energy', 'Danceability'], net_name='EarlyMultiCNNTrain_beatMEL', PATIENCE=PATIENCE, cnn=True, multi=True)



"""##### Show Loss Weights Convergence"""

print(criterion)

with open('./best_EarlyMultiCNNTrain_beatMEL.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, cnn=True, multi=True, titles=['Valence', 'Energy', 'Danceability'])

"""### Multi CNN Training with constant weights

#### Redefine MultiTask Regresion Parameters
"""

out_features = 1
task_num = 3
regularization_strength = 1e-2
loss = SpearmanLoss(regularization_strength=regularization_strength)
weights = [1, 1, 1]
criterion = MultiTaskLoss(task_num)

"""#### Modeling Pipeline"""

data = SpectrogramDataset("data/multitask_dataset_beat/",
                              class_mapping=class_mapping, train=True, read_spec_fn=read_mel_spectrogram, emotion=np.array(list(mapping.values())))
train_loader, dev_loader = torch_train_val_split(data, batch_size , batch_size, val_size=val_size)
test_data = SpectrogramDataset('data/multitask_dataset_beat/',
                                          class_mapping=class_mapping, train=False, read_spec_fn=read_mel_spectrogram, emotion=np.array(list(mapping.values())))

"""##### Overfit the model

"""

EarlyCNNNet = MultiCNN(layer_channels, hidden_features, out_features, kernels).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(EarlyCNNNet.parameters(), lr=ETA, weight_decay=weight_decay)

reg_main(device, EarlyCNNNet, mels_feats, optimizer, criterion, train_loader, train_loader, 30, Descrs = ['Valence', 'Energy', 'Danceability'], PATIENCE=PATIENCE, overfit_batch=True, cnn=True, multi=True)

"""##### Normal Training"""

EarlyCNNNet = MultiCNN(layer_channels, hidden_features, out_features, kernels).to(device)

# Make instance of our optimizer, with L2-Reguralization
optimizer = optim.Adam(parameters, lr=ETA, weight_decay=weight_decay)

# Start the training process
reg_main(device, EarlyCNNNet, mels_feats, optimizer, criterion, train_loader, dev_loader, EPOCHS, Descrs = ['Valence', 'Energy', 'Danceability'], net_name='EarlyMultiCNNConst_beatMEL', PATIENCE=PATIENCE, cnn=True, multi=True)

with open('./best_EarlyMultiCNNConst_beatMEL.pickle', 'rb') as handle:
    E_model = pickle.load(handle)

# test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)
_ = reg_eval(device, dev_loader, mels_feats, E_model, criterion, cnn=True, multi=True, titles=['Valence', 'Energy', 'Danceability'])